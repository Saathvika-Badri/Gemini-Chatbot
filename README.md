This project is a lightweight Streamlit-based chatbot application that integrates Google’s Gemini API to generate intelligent, contextual responses. The core structure is organized into a main application script, a separate client module to handle API communication, a requirements file for dependency management, and an environment file to securely store the API key. The main script initializes Streamlit’s session state to maintain conversational memory within the session and uses Streamlit’s chat components to render messages in a clean chat interface. When the user inputs a message, the full conversation history stored in session state is concatenated into a single prompt, which is then passed to the Gemini client function. The gemini_client module loads the API key from the environment, configures the Google Generative AI client, and sends the prompt to the Gemini model, returning the generated response back to the interface. The system is designed so that all model communication happens exclusively through this utility file, keeping the architecture modular and easy to modify. Streamlit automatically refreshes the UI with every message but preserves the context using session state, giving the impression of a continuous chat. The application depends on a few core packages—Streamlit for UI rendering, python-dotenv for environment variables, and Google’s generativeai library for accessing the model—making the project both portable and simple to deploy. Overall, the project combines clean modular design with Streamlit’s reactive framework to deliver a functional AI chatbot that maintains session-based memory and responds intelligently through Gemini.
